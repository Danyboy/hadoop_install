#!/bin/bash

hadoop_user="hadoop"
root_dir="/opt"
hadoop_dir="${root_dir}/hadoop"

SCRIPTNAME="$(basename $0)"
MYSCRIPTDIR=$(dirname "$0")
[ "$MYSCRIPTDIR" = "." ] && MYSCRIPTDIR="$(pwd)"
MYHOSTS=$MYSCRIPTDIR/hosts
VERBOSE=true
[ "$EUID" = 0 ] && MYSUDO="" || MYSUDO="sudo"


currenthost="$(hostname)"
tmpdir="/tmp"
master_name="$(cat ${MYHOSTS} | head -n1 | cut -f 2 -d " ")"
master_node="hdfs://$master_name:9000/" #master_node="hdfs://hmaster:9000/"

add_hosts(){
    my_echo "Host installing $currenthost"
    [ ! -e "$MYHOSTS" ] && my_echo "Error, where is hosts file? $MYHOSTS" && return 1
    cat $MYHOSTS >> /etc/hosts
}

run_on_all_host(){
    [ ! -e "$MYHOSTS" ] && my_exit "Fatal, where is hosts file? $MYHOSTS"
    while read ip chost ; do 
	my_echo "Copy script on $chost"
	scp -r ${MYSCRIPTDIR} ${chost}:${tmpdir}/
	ssh -tt $chost sed -i.bak '/Defaults    requiretty/d' /etc/sudoers

	#ssh $chost ${tmpdir}/$(basename ${MYSCRIPTDIR})/${SCRIPTNAME} -i
	#ssh $chost yum install -y sudo #TODO disable
	my_echo "Run script on $chost"
	ssh -tt $chost ${MYSUDO} ${tmpdir}/${MYSCRIPTDIR}/${SCRIPTNAME} -i

	#ssh $chost ${MYSUDO} 'bash /dev/stdin -i' <<< "curl -s https://raw.githubusercontent.com/Danyboy/hadoop_install/master/hadoop_install"
    done < <(cat $MYHOSTS)
}

user_creation(){
    useradd $hadoop_user
    #passwd $hadoop_user
}

generate_key(){
    MYKEY="${HOME}/.ssh/id_rsa"
    MYAUTH="${HOME}/.ssh/authorized_keys"

    [ -e "$MYKEY" ] && [ -e "$MYAUTH" ] && my_echo "SSH keys already exists." && return 0
    ssh-keygen -t rsa -f ${MYKEY} -q -N ""
    cat ${MYKEY}.pub >> ${MYAUTH}
    chmod 600 ${MYAUTH}

cat <<EOF >> ${HOME}/.ssh/config

Host *
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
EOF

chmod 644 ${HOME}/.ssh/config
}

copy_keys(){
    while read ip chost ; do
	scp -r ~/.ssh/ ${chost}:${tmpdir}/
	ssh -tt ${chost} ${MYSUDO} cp -a ${tmpdir}/.ssh/ /home/$hadoop_user/
	ssh -tt ${chost} ${MYSUDO} chown -R $hadoop_user:$hadoop_user /home/$hadoop_user/
	ssh -tt ${chost} ${MYSUDO} rm -rf ${tmpdir}/.ssh
    done < <(cat $MYHOSTS)
}

java_install(){
    my_echo "Java installing $currenthost"
    yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel which openssh-clients openssh-server net-tools sudo mc wget
    #curl wget 
}

install_hadoop(){

    my_echo "Hadoop installing $currenthost"

    #Installing and configuring hadoop 2.6.0
    #On master:

    if [ ! -e "${hadoop_dir}" ] ; then
        cd ${root_dir}
	wget http://www.eu.apache.org/dist/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
    	tar -zxf hadoop-2.6.0.tar.gz
	rm hadoop-2.6.0.tar.gz
	mv hadoop-2.6.0 hadoop
    else
	my_echo "Hadoop dir already exist, hadoop install skipped."
    fi

    #Run from hadoop or to hadoop user

cat <<EOF >>/home/$hadoop_user/.bashrc
export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:/bin/java::")
export HADOOP_PREFIX=${hadoop_dir}
export HADOOP_HOME=\$HADOOP_PREFIX
export HADOOP_COMMON_HOME=\$HADOOP_PREFIX
export HADOOP_CONF_DIR=\$HADOOP_PREFIX/etc/hadoop
export HADOOP_HDFS_HOME=\$HADOOP_PREFIX
export HADOOP_MAPRED_HOME=\$HADOOP_PREFIX
export HADOOP_YARN_HOME=\$HADOOP_PREFIX
export PATH=\$PATH:\$HADOOP_PREFIX/sbin:\$HADOOP_PREFIX/bin
EOF

}

config_hadoop(){

my_echo "Configure hadoop $currenthost"

#Edit ${hadoop_dir}/etc/hadoop/core-site.xml – set up NameNode URI on every node:

cat <<EOF >${hadoop_dir}/etc/hadoop/core-site.xml
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>$master_node</value>
</property>
</configuration>
EOF

#Create HDFS DataNode data dirs on every node and change ownership of ${hadoop_dir}:

chown $hadoop_user:$hadoop_user ${hadoop_dir}/ -R
mkdir /home/$hadoop_user/datanode
chown $hadoop_user:$hadoop_user /home/$hadoop_user/datanode/

#Edit ${hadoop_dir}/etc/hadoop/hdfs-site.xml – set up DataNodes:

cat <<EOF >${hadoop_dir}/etc/hadoop/hdfs-site.xml
<configuration>
<property>
  <name>dfs.replication</name>
  <value>3</value>
</property>
<property>
  <name>dfs.permissions</name>
  <value>false</value>
</property>
<property>
   <name>dfs.datanode.data.dir</name>
   <value>/home/$hadoop_user/datanode</value>
</property>
</configuration>
EOF
}

config_hadoop_master(){
my_echo "Configure hadoop master $currenthost"

#Create HDFS NameNode data dirs on master:

mkdir /home/$hadoop_user/namenode
chown $hadoop_user:$hadoop_user /home/$hadoop_user/namenode/

#Edit ${hadoop_dir}/etc/hadoop/hdfs-site.xml on master. Add further properties:

cat <<EOF >${hadoop_dir}/etc/hadoop/hdfs-site.xml
<configuration>
<property>
        <name>dfs.namenode.data.dir</name>
        <value>/home/$hadoop_user/namenode</value>
</property>
</configuration>
EOF

#Edit ${hadoop_dir}/etc/hadoop/mapred-site.xml on master.

cat <<EOF >${hadoop_dir}/etc/hadoop/mapred-site.xml
<configuration>
 <property>
  <name>mapreduce.framework.name</name>
   <value>yarn</value> <!-- and not local (!) -->
 </property>
</configuration>
EOF

#TODO check yarn config on slaves
#Edit ${hadoop_dir}/etc/hadoop/yarn-site.xml – setup ResourceManager and NodeManagers:

cat <<EOF >${hadoop_dir}/etc/hadoop/yarn-site.xml
<configuration>
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>${master_name}</value>
</property>
<property>
        <name>yarn.nodemanager.hostname</name>
        <value>${master_name}</value> <!-- or hslave1, hslave2, hslave3 -->
</property>
<property>
  <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
</configuration>
EOF

#Edit ${hadoop_dir}/etc/hadoop/slaves on master (so that master may start all necessary services on slaves automagically):
cat $MYHOSTS | cut -f 2 -d " " > ${hadoop_dir}/etc/hadoop/slaves

}
start_hadoop(){
#Now the important step: disable firewall and IPv6 (Hadoop does not support IPv6 – problems with listening on all the interfaces via 0.0.0.0):

# systemctl stop firewalld
#Add the following lines to /etc/sysctl.conf:
#net.ipv6.conf.all.disable_ipv6 = 1
#net.ipv6.conf.default.disable_ipv6 = 1

generate_key

#Format NameNode:
hdfs namenode -format

#hadoop-env.sh
#MYENV="${hadoop_dir}/etc/hadoop/hadoop-env.sh"
#chmod 755 $MYENV
#$MYENV
#Start HDFS (as user hadoop):
start-dfs.sh

#Check out with jps if DataNode are running on slaves and if DataNode, NameNode, and SecondaryNameNode are running on master. Also try accessing http://hmaster:50070/
#Start YARN on master:

start-yarn.sh 

#Now NodeManagers should be alive (jps) on all nodes and a ResourceManager on master too.
#We see that the master node consists of a ResourceManager, NodeManager (YARN), NameNode and DataNode (HDFS). A slave node acts as both a NodeManager and a DataNode.
}

test_hadoop(){
my_echo "Testing hadoop 2.6.0"

#You may want to check out if you are able to copy a local file to HDFS and run the standalone Hadoop Hello World (i.e. wordcount) Job.

#$ hdfs dfsadmin -safemode leave # ??????
#$ hdfs dfs -mkdir /input
#$ hdfs dfs -copyFromLocal test.txt /input
#$ hdfs dfs -cat /input/test.txt | head
#$ hadoop jar ${hadoop_dir}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /input/test.txt /output1
#If anything went wrong, check out ${hadoop_dir}/log/*.log. Good luck :)
#‹ stringi 0.4-1 released – fast, portable, consistent character string processingUsing Hadoop Streaming API to perform a word count job in R and C++ ›
}

enable_ssh(){
    yum install -y openssh-clients openssh-server
    ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key -q -N ""
    ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -q -N ""
    ssh-keygen -t rsa1 -f /etc/ssh/ssh_host_rsa_key -q -N ""
    /usr/sbin/sshd -D &
}

pub_keys(){
    cat ~/.ssh/id_rsa.pub >> /home/danil/Projects/bash/hadoop_conf/authorized_keys
}

copy_my_keys(){
    cp /home/danil/Projects/bash/hadoop_conf/authorized_keys ~/.ssh/authorized_keys
    chmod 600 ~/.ssh/authorized_keys
    cat ~/.ssh/authorized_keys
}


my_echo(){
    $VERBOSE && echo "$1"
}

my_exit(){
    $VERBOSE && echo "$1"
    exit $RETVAL
}

usage(){
    echo " $SCRIPTNAME -<i|h|a>"
    echo " Intsall hadoop on hosts or on a current machine"
    echo ""
    echo " example: # $SCRIPTNAME -i # for install hadoop on current machine"
    echo " example: # $SCRIPTNAME -a # for install hadoop on all machines in hosts file"
    my_exit
}

my_start(){
    ${MYSUDO} su -c "${MYSCRIPTDIR}/${SCRIPTNAME} -s" ${hadoop_user}
    
    #chmod a+rx "${MYSCRIPTDIR}/${SCRIPTNAME}"
    #su -c "${MYSCRIPTDIR}/${SCRIPTNAME} -r" - ${hadoop_user}
}

my_install(){
    add_hosts
    user_creation
    java_install
    install_hadoop
    config_hadoop
}

my_install_master(){
    my_install
    config_hadoop_master
    enable_ssh #TODO disable or check
    generate_key
    my_start
}

while getopts “hiasrmegpcd” OPTION 
do
     case $OPTION in
         h)
             usage
             exit 1
             ;;
         i)
             my_install
             exit 1
             ;;
         a)
	     add_hosts || my_exit "Hosts installation error."
	     run_on_all_host #|| my_exit "Error Hadoop installation on slave hosts."
	     copy_keys || my_exit "Ssh keys copy error."
	     config_hadoop_master
	     my_start
             exit 1
             ;;
         s)
             my_start
             exit 1
             ;;
         r)
             start_hadoop
             exit 1
             ;;
         m)
             my_install_master
             exit 1
             ;;
         e)
	     enable_ssh
             exit 1
             ;;
         g)
	     enable_ssh
	     generate_key
             exit 1
             ;;
         p)
	     pub_keys
             exit 1
             ;;
         c)
	     copy_my_keys
             exit 1
             ;;
         d)
	     add_hosts
             exit 1
             ;;
         ?)
             usage
             exit
             ;;
         *)
             usage
             exit
             ;;
     esac
done

if [ $OPTIND -eq 1 ]; then usage; fi

#TODO
#Config
#systemd service
#checking opt/hadoop, hosts
#http://www.rexamine.com/2015/02/installing-hadoop-2-6-0-on-centos-7/


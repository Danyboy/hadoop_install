#!/bin/bash

#http://www.rexamine.com/2015/02/installing-hadoop-2-6-0-on-centos-7/

hadoop_user="hadoop"
root_dir="/opt"
hadoop_dir="${root_dir}/hadoop"
master_name="$(hostname)"
master_node="hdfs://$master_name:9000/" #master_node="hdfs://hmaster:9000/"

SCRIPTNAME="$(basename $0)"
MYSCRIPTDIR=$(dirname "$0")
[ "$MYSCRIPTDIR" = "." ] && MYSCRIPTDIR="$(pwd)"
MYHOSTS=$MYSCRIPTDIR/hosts
VERBOSE=true

currenthost="$(hostname)"
tmpdir="/tmp"

add_hosts(){
    my_echo "Host installing $currenthost"
    [ ! -e "$MYHOSTS" ] && my_echo "Error, where is hosts file? $MYHOSTS"
    cat $MYHOSTS >> /etc/hosts
}

run_on_all_host(){
    [ ! -e "$MYHOSTS" ] && my_exit "Fatal, where is hosts file? $MYHOSTS"
    while read ip chost ; do 
	scp -r ${MYSCRIPTDIR} ${chost}:${tmpdir}/
	ssh $chost sudo ${tmpdir}/${MYSCRIPTDIR}/${SCRIPTNAME} -i
	#ssh $chost sudo 'bash /dev/stdin -i' <<< "curl -s https://raw.githubusercontent.com/Danyboy/hadoop_install/master/hadoop_install"
    done < <(cat $MYHOSTS)
}

user_creation(){
    useradd $hadoop_user
    #passwd $hadoop_user
}

copy_keys(){
    # su $hadoop_user
    # ssh-keygen -t rsa
    # chmod 0600 ~/.ssh/authorized_keys
    #ssh-keygen -t ed25519

    while read ip chost ; do
	scp -r ~/.ssh/ ${chost}:${tmpdir}/
	ssh ${chost} sudo cp -a ${tmpdir}/.ssh/ /home/$hadoop_user/
	ssh ${chost} sudo chown -R $hadoop_user:$hadoop_user /home/$hadoop_user/
	ssh ${chost} sudo rm -rf ${tmpdir}/.ssh
    done < <(cat $MYHOSTS)
}

java_install(){
    my_echo "Java installing $currenthost"
    yum install -y java-1.8.0-openjdk curl wget
}

install_hadoop(){

    my_echo "Hadoop installing $currenthost"

    #Installing and configuring hadoop 2.6.0
    #On master:

    if [ ! -e "${hadoop_dir}" ] ; then
        cd ${root_dir}
	wget http://www.eu.apache.org/dist/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
    	tar -zxf hadoop-2.6.0.tar.gz
	rm hadoop-2.6.0.tar.gz
	mv hadoop-2.6.0 hadoop
    else
	my_echo "Hadoop dir already exist, hadoop install canceled."
    fi

    #Run from hadoop or to hadoop user

cat <<EOF >>/home/$hadoop_user/.bashrc
export HADOOP_PREFIX=${hadoop_dir}
export HADOOP_HOME=$HADOOP_PREFIX
export HADOOP_COMMON_HOME=$HADOOP_PREFIX
export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
export HADOOP_HDFS_HOME=$HADOOP_PREFIX
export HADOOP_MAPRED_HOME=$HADOOP_PREFIX
export HADOOP_YARN_HOME=$HADOOP_PREFIX
export PATH=$PATH:$HADOOP_PREFIX/sbin:$HADOOP_PREFIX/bin
EOF

}

config_hadoop(){

my_echo "Configure hadoop $currenthost"

#Edit ${hadoop_dir}/etc/hadoop/core-site.xml – set up NameNode URI on every node:

cat <<EOF >${hadoop_dir}/etc/hadoop/core-site.xml
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>$master_node</value>
</property>
</configuration>
EOF

#Create HDFS DataNode data dirs on every node and change ownership of ${hadoop_dir}:

chown $hadoop_user:$hadoop_user ${hadoop_dir}/ -R
mkdir /home/$hadoop_user/datanode
chown $hadoop_user:$hadoop_user /home/$hadoop_user/datanode/

#Edit ${hadoop_dir}/etc/hadoop/hdfs-site.xml – set up DataNodes:

cat <<EOF >${hadoop_dir}/etc/hadoop/hdfs-site.xml
<configuration>
<property>
  <name>dfs.replication</name>
  <value>3</value>
</property>
<property>
  <name>dfs.permissions</name>
  <value>false</value>
</property>
<property>
   <name>dfs.datanode.data.dir</name>
   <value>/home/$hadoop_user/datanode</value>
</property>
</configuration>
EOF
}

config_hadoop_master(){
my_echo "Configure hadoop master $currenthost"

#Create HDFS NameNode data dirs on master:

mkdir /home/$hadoop_user/namenode
chown $hadoop_user:$hadoop_user /home/$hadoop_user/namenode/

#Edit ${hadoop_dir}/etc/hadoop/hdfs-site.xml on master. Add further properties:

cat <<EOF >${hadoop_dir}/etc/hadoop/hdfs-site.xml
<configuration>
<property>
        <name>dfs.namenode.data.dir</name>
        <value>/home/$hadoop_user/namenode</value>
</property>
</configuration>
EOF

#Edit ${hadoop_dir}/etc/hadoop/mapred-site.xml on master.

cat <<EOF >${hadoop_dir}/etc/hadoop/mapred-site.xml
<configuration>
 <property>
  <name>mapreduce.framework.name</name>
   <value>yarn</value> <!-- and not local (!) -->
 </property>
</configuration>
EOF

#Edit ${hadoop_dir}/etc/hadoop/yarn-site.xml – setup ResourceManager and NodeManagers:

cat <<EOF >${hadoop_dir}/etc/hadoop/yarn-site.xml
<configuration>
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>${master_name}</value>
</property>
<property>
        <name>yarn.nodemanager.hostname</name>
        <value>${master_name}</value> <!-- or hslave1, hslave2, hslave3 -->
</property>
<property>
  <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
</configuration>
EOF

#Edit ${hadoop_dir}/etc/hadoop/slaves on master (so that master may start all necessary services on slaves automagically):
cat $MYHOSTS | cut -f 2 -d " " > ${hadoop_dir}/etc/hadoop/slaves

}

my_echo(){
    $VERBOSE && echo "$1"
}

my_exit(){
    $VERBOSE && echo "$1"
    exit $RETVAL
}

usage(){
    echo " $SCRIPTNAME -<i|h|a>"
    echo " Intsall hadoop on hosts or current machine"
    echo ""
    echo " example: # $SCRIPTNAME -i # for install hadoop on current machine"
    echo " example: # $SCRIPTNAME -a # for install hadoop on all machines in hosts file"
    my_exit
}

while getopts “hia” OPTION 
do
     case $OPTION in
         h)
             usage
             exit 1
             ;;
         i)
	     add_hosts
	     user_creation
	     java_install
	     install_hadoop
	     config_hadoop
             exit 1
             ;;
         a)
	     add_hosts
	     run_on_all_host
	     copy_keys
	     config_hadoop_master
             exit 1
             ;;
         ?)
             usage
             exit
             ;;
         *)
             usage
             exit
             ;;
     esac
done

#TODO
#Config
#systemd service
#checking opt/hadoop, hosts

#Big comments
: <<'END'
start_hadoop(){
#Now the important step: disable firewall and IPv6 (Hadoop does not support IPv6 – problems with listening on all the interfaces via 0.0.0.0):

# systemctl stop firewalld
#Add the following lines to /etc/sysctl.conf:
#net.ipv6.conf.all.disable_ipv6 = 1
#net.ipv6.conf.default.disable_ipv6 = 1
#Format NameNode:

# su hadoop
$ hdfs namenode -format
Start HDFS (as user hadoop):

$ start-dfs.sh
Check out with jps if DataNode are running on slaves and if DataNode, NameNode, and SecondaryNameNode are running on master. Also try accessing http://hmaster:50070/

Start YARN on master:

$ start-yarn.sh 
Now NodeManagers should be alive (jps) on all nodes and a ResourceManager on master too.

We see that the master node consists of a ResourceManager, NodeManager (YARN), NameNode and DataNode (HDFS). A slave node acts as both a NodeManager and a DataNode.

Testing hadoop 2.6.0

You may want to check out if you are able to copy a local file to HDFS and run the standalone Hadoop Hello World (i.e. wordcount) Job.

$ hdfs dfsadmin -safemode leave # ??????
$ hdfs dfs -mkdir /input
$ hdfs dfs -copyFromLocal test.txt /input
$ hdfs dfs -cat /input/test.txt | head
$ hadoop jar ${hadoop_dir}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /input/test.txt /output1
If anything went wrong, check out ${hadoop_dir}/log/*.log. Good luck :)

‹ stringi 0.4-1 released – fast, portable, consistent character string processingUsing Hadoop Streaming API to perform a word count job in R and C++ ›
Tagged with: CentOS, Hadoop, HDFS, YARN
}
END
